\documentclass[12pt]{llncs}

\usepackage[utf8]{inputenc}
\usepackage[spanish,activeacute]{babel}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[colorlinks, citecolor=black, urlcolor=black, bookmarks=false, hypertexnames=true]{hyperref} 
\usepackage{url}
\usepackage{listings}
\usepackage{color}

\begin{document}

\begin{center}
\large{
\rule{\textwidth}{0.5pt}
\par Universidad de la Habana, MATCOM
\par Sistemas de Recuperación de Información (2022)
\vspace{0.4cm}
\par \textcolor{cyan}{Informe de Entrega Parcial}
\par Samuel David Suárez Rodríguez C512
\par Enmanuel Verdesia Suárez C511
\par Gabriel Fernando Martín Fernández C511
\rule{\textwidth}{1.5pt}
}

%\par \textbf{Tema:} Búsqueda empleando el modelo vectorial.
\end{center}

\section{Descripción General}
\vspace{0.5cm}
\subsection{Resumen}
\par Se pretende desarrollar un Sistema de Recuperación de Información (SRI) con el apoyo de técnicas de Inteligencia Artificial (IA). Dado un conjunto de varios documentos de diversos temas se desarrollará una aplicación de búsqueda que obtenga resultados relevantes y acordes a una consulta presentada por el usuario. Además se emplearán métodos para clasificar los documentos y extraer sus características más relevantes usando algoritmos para la clasificación y selección de características. Todos estos procesos de clasificación se apoyarán en el uso de algoritmos de machine learning (ML) para lograr su objetivo.

\subsection{Estructura del proyecto}
\par El proyecto presenta tres componentes principales: el backend o componente lógica de la aplicación, junto con la API, y el frontend en forma de un servicio web.

Está desarrollado en GitHub bajo la organización \href{https://github.com/Gologle}{\color{blue}Gologle}. Ahí pueden ser encontrados los dos repositorios principales. Además están hospedados el cliente del sistema y la API en internet.

\begin{itemize}
  \item Web: https://gologle.vercel.app
  \item API: https://gologle-api.herokuapp.com/
\end{itemize}

\section*{Detalles de implementación}

\section{Procesamiento de los sets de datos}

Para cada set de datos empleado fue implementado un parser para extraer su información. Este proceso no se pudo realizar de forma común pues cada set contenía los documentos de forma particular. Una vez \textit{parseados} los sets de datos se puede delimitar e identificar cada documento que contiene de forma única por un ID y así guardar el texto de los documentos en una base de datos SQLite. Dicha base de datos facilita obtener un menor tiempo de respuesta ante un pedido de parte del cliente.

Los sets de datos con los que se trabajó fueron Cranfield (1400 documentos), Newsgroups (18828 documentos) y Reuters (????? documentos). Se contaba con consultas y resultados relevantes para Cranfield. {\color{red}Que se tiene en Newsgroups y/o Reuters?}

\section{Modelo Vectorial (TF-IDF)}

Dado que ha sido implementado el modelo vectorial, haciendo uso de la biblioteca \verb+sklearn+ nos ajustamos a la forma que esta requiere para representar los documentos. Dicha biblioteca se usó para tokenizar los documentos y extraer como \textit{features} los términos que estos contienen. El texto de los documentos es llevado a minúscula, además los términos de un simple carácter son eliminados. Así se obtiene cada documento como un vector de términos (palabras) con una dimensión determinada.

A partir del conjunto de documentos se obtiene una matriz esparcida que representa la cantidad de veces que aparece el término $i$ en el documento $j$. Sobre esta matriz nos apoyamos para calcular la frecuencia de documentos inversa ($idf$) para cada término.

$$idf_i = \log \frac{N}{n_i}$$

$n_i$: cantidad de documentos en los que aparece el término $i$.

Luego haciendo uso de la clase \verb+TfidfTransformer+ que se encuentra en el módulo \verb+sklearn.feature_extraction.text+ calculamos los pesos para cada término en cada documento, de acuerdo a la expresión:

$$w_{ij} = tf_{ij} * idf_i$$

donde $t_{ij}$ es la frecuencia normalizada del término $i$ en el document $j$,

$$tf_{ij} = \frac{freq_{ij}}{max_l freq_{lj}}$$

Hecho esto, se tiene la matriz esparcida de los pesos, la cual es guardada en la base de datos para agilizar el tiempo de respuesta.

La base de datos creada a partir de los documentos, los términos, la $idf$ para cada término y los pesos es considerada el índice del sistema para satisfacer las consultas de la forma mas rápida posible.

\subsection{Recuperación de Información}

Cuando es recibida una consulta $q$, esta es procesada de forma similar a como se procesa un documento. Se calcula el peso para cada término ($w_{iq}$), mediante la siguiente expresión

$$w_{iq} = (\alpha + (1 - \alpha) \frac{freq_{lq}}{max_l freq_{lq}}) * idf_i$$

donde $\alpha$ es un valor de suavizado que permite amortiguar la contribución de la frecuencia de los términos, se le asignó un valor de 0.5.

La similitud entre el vector obtenido y los documentos de la base de datos es calculada usando como valor de referencia el coseno del ángulo entre estos. Los documentos con mayor similitud son dados como resultado de forma ordenada.


\section{Modelo Vectorial (Doc2Vec)}


\end{document}